Metadata-Version: 2.4
Name: plato
Version: 0.1.0
Summary: Local-first PDF to Knowledge Graph & Semantic Search Pipeline
Author: Darleison Filho
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: networkx
Requires-Dist: pydantic>=2.0
Requires-Dist: pyyaml
Requires-Dist: ollama
Requires-Dist: matplotlib
Requires-Dist: spacy
Requires-Dist: pandas
Requires-Dist: typer
Requires-Dist: rich
Requires-Dist: llama-index-core
Requires-Dist: llama-index-embeddings-huggingface
Requires-Dist: llama-index-llms-ollama

![Platograph Logo](logo.png)

```ascii
               d8b                                       
               88P                      d8P              
              d88                    d888888P            
?88,.d88b,    888       d888b8b        ?88'       d8888b 
`?88'  ?88    ?88      d8P' ?88        88P       d8P' ?88
  88b  d8P     88b     88b  ,88b       88b       88b  d88
  888888P'      88b    `?88P'`88b      `?8b      `?8888P'
  88P'                                                   
 d88                                                     
 ?8P    
```

> *A platypus approach to research knowledge graphs.*

**PLATO** is a local-first **Context Preparation Assistant** for independent researchers, product managers, and developers. Like the platypus‚Äîa unique hybrid of different animals‚ÄîPLATO combines **PDF processing**, **Vector Search**, and **Knowledge Graphs** into one cohesive tool.

an intelligent local-first document processor that creates
  structured knowledge for you to chat with and use in other projects.


## Why "PLATO"?
Just as the platypus is one of nature's most unique creatures, your research deserves a unique approach. We don't try to build a "genius AGent" that does the thinking for you. Instead, we use lightweight (1B) models to **prepare the context** so *you* (or a larger model) can do the deep work.

- ü¶´ **Hybrid Intelligence**: Vector search + Knowledge graphs.
- üè† **Privacy-first**: All processing happens locally on your machine (via Ollama).
- üß© **The Contexter Pattern**: Efficient resource management for your PDFs and models.
- ‚ö° **Lightweight**: Optimized for consumer hardware (M1/M2 Macs with 8GB RAM).

## The Workflow
1.  **Scan**: Dump your PDFs into a folder. Platograph scans them using a fast 1B model to understand what they are.
2.  **Suggest**: The agent looks at your collection and suggests workflows (e.g., "Build a comparison table," "Extract timeline").
3.  **Build**: You approve, and Platograph constructs a structured `context.md` file properly 

> A local-first, schema-driven knowledge extraction tool.

**Platograph** is an intelligent document processor designed for researchers, developers, and product managers. It creates a local, self-hosted knowledge base from your documents and lets you interact with it through a conversational chat interface, all powered by local LLMs via Ollama.

The goal of Platograph is to provide a frictionless, "plug and play" experience for turning a folder of documents into a queryable knowledge graph.

## Core Features

- **üè† Local First & Private**: All processing and data storage happens on your machine. Your documents never leave your computer.
- **üß† Intelligent Extraction**: Uses local LLMs to build a knowledge graph and vector index from your documents, enabling both semantic search and structured queries.
- **üí¨ Conversational Interface**: Use the `chat` command to ask questions and get answers directly from your knowledge base.
- **üß© Simple & File-Based**: No external databases or complex setup required. The knowledge graph is stored in your project directory.
- **‚öôÔ∏è Configurable Models**: Easily configure which Ollama models to use for different tasks (e.g., a powerful model for extraction, a fast model for chat).

## Quick Start

**1. Prerequisites**

Ensure you have [Ollama](https://ollama.com) installed and running.

**2. Install Models**

Pull the default models that Platograph uses. You can change these later in `config.yaml`.

```bash
ollama pull embedding-gemma
ollama pull qwen2.5-coder
ollama pull dolphin-phi
```

**3. Install Platograph**

Clone the repository and install it in editable mode.

```bash
git clone <repository_url>
cd plato
pip install -e .
```

**4. Process Your Documents**

Place your documents (PDFs, etc.) into a directory (e.g., `my_papers/`) and run the `process` command.

```bash
plato process ./my_papers/
```

Platograph will read the documents, build the knowledge graph, and store it locally in the `./storage` directory.

**5. Chat With Your Knowledge**

Start a conversation with your newly created knowledge base.

```bash
plato chat
```

---

## Command Reference

### `plato process <directory>`

This is the core command for ingesting documents.

- It takes one argument: the path to the directory containing your documents.
- It recursively finds all supported files, parses them, and builds/updates your local knowledge graph and vector store.

### `plato chat`

Starts an interactive chat session with the knowledge base you've built.

- Ask questions in natural language.
- Type `exit` or `quit` to end the session.

---
*Built with ‚ù§Ô∏è for the curious.*
